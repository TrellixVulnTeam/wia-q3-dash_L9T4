{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path\n",
    "BASE_PATH = pathlib.Path(\"testing.ipynb\").parent.resolve()\n",
    "DATA_PATH = BASE_PATH.joinpath(\"data\").resolve()\n",
    "\n",
    "attendees = pd.read_csv(DATA_PATH.joinpath(\"Q3-2019 Attendees.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "attendees.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_analysis_work = attendees.copy()\n",
    "\n",
    "q3_analysis_work = q3_analysis_work.rename(columns = {'Order #':'order#', 'Ticket Type':'ticket_type', 'Attendee #':'attendee#', 'Gender': 'gender', \n",
    "                                                      'City': 'city', 'Zip Code': 'zipcode', 'Country': 'country', \n",
    "                                                      'Dietary Restrictions': 'dietary_restrictions', 'Technical Expertise': 'technical_expertise',\n",
    "                                                      'Analytics Experience': 'analytics_experience', 'Job Function': 'job_function', 'Industry': 'industry',\n",
    "                                                      'What topics would you like to hear about?': 'topics_like_hear',\n",
    "                                                      'Are you interested in being recruited by sponsoring companies?': 'interested_being_recruited',\n",
    "                                                      'Do you plan on attending the opening reception on Wednesday, June 3rd, 2020?': 'plan_attending_reception',\n",
    "                                                      'Did someone refer you to this conference? If so, please list their name.': 'someone_referred_ifso_name',\n",
    "                                                      'Additional Terms': 'additional_terms', 'Job Title': 'job_title', 'Company': 'company'})\n",
    "col = ['ticket_type', 'attendee#', 'gender', 'city', 'zipcode',\n",
    "       'country', 'dietary_restrictions', 'technical_expertise',\n",
    "       'analytics_experience', 'job_function', 'industry', 'topics_like_hear',\n",
    "       'interested_being_recruited', 'plan_attending_reception',\n",
    "       'someone_referred_ifso_name', 'additional_terms', 'job_title',\n",
    "       'company']\n",
    "for i in col:\n",
    "    q3_analysis_work[col] = q3_analysis_work[col].replace(np.nan,'No Answer')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_analysis_work.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import random\n",
    "from wordcloud import ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Listed of stop-words found at\n",
    "# https://pypi.python.org/pypi/stop-words\n",
    "\n",
    "stop_words = get_stop_words('en')\n",
    "\n",
    "# Clean up text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texts = []\n",
    "for i in q3_analysis_work['job_title'].dropna(how='all'):\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    longer_tokens = [i for i in stopped_tokens if len(i) > 2]\n",
    "    texts.append(longer_tokens)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Run tf-idf\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "##WIA\n",
    "\n",
    "# Based off example:\n",
    "# https://github.com/amueller/word_cloud/blob/master/examples/a_new_hope.py\n",
    "\n",
    "def red_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(24, 99%%, %d%%)\" % random.randint(40, 70)\n",
    "\n",
    "ASSETS_PATH = BASE_PATH.joinpath(\"assets\").resolve()\n",
    "\n",
    "wia_mask = np.array(Image.open(ASSETS_PATH.joinpath(\"WIA_logo.jpg\")))\n",
    "#top_words = np.sort(np.array(tfidf[corpus[57]],dtype = [('word',int), ('score',float)]),order='score')[::-1]\n",
    "\n",
    "wc = WordCloud(background_color=\"white\",mask=wia_mask,random_state=5,max_words=2000).generate(str(texts))\n",
    "#.fit_words(dict([(dictionary[word],score) for word,score in top_words])))\n",
    "\n",
    "plt.imshow(wc.recolor(color_func=red_color_func, random_state=5),interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "wc.to_file(\"wia_title_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from operator import itemgetter\n",
    "WNL = nltk.WordNetLemmatizer()\n",
    "\n",
    "q3_analysis_work['job_title'] = q3_analysis_work['job_title'].str.replace(' / ', '')\n",
    "q3_analysis_work['job_title'] = q3_analysis_work['job_title'].str.replace('Sr', 'Senior')\n",
    "\n",
    "df2 = q3_analysis_work['job_title'].dropna(how='all').astype(str)\n",
    "df2 = \" \".join(job_title for job_title in df2)\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"I\", \"on\", \"and\", \"good\", \"etc\", \"make\", \"better\", \"depending\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "text_content = [WNL.lemmatize(str(df2)) for t in df2]\n",
    "\n",
    "tokens = nltk.word_tokenize(str(df2))\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# Remove extra chars and remove stop words.\n",
    "text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text]\n",
    "text_content = [word for word in text_content if word not in stopwords]\n",
    " \n",
    "# After the punctuation above is removed it still leaves empty entries in the list.\n",
    "# Remove any entries where the len is zero.\n",
    "text_content = [s for s in text_content if len(s) != 0]\n",
    " \n",
    "# Best to get the lemmas of each word to reduce the number of similar words\n",
    "# on the word cloud. The default lemmatize method is noun, but this could be\n",
    "# expanded.\n",
    "# ex: The lemma of 'characters' is 'character'.\n",
    "text_content = [WNL.lemmatize(t) for t in text_content]\n",
    " \n",
    "# setup and score the bigrams using the raw frequency.\n",
    "finder = BigramCollocationFinder.from_words(text_content)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "\n",
    "\n",
    "# By default finder.score_ngrams is sorted, however don't rely on this default behavior.\n",
    "# Sort highest to lowest based on the score.\n",
    "scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    " \n",
    "# word_dict is the dictionary we'll use for the word cloud.\n",
    "# Load dictionary with the FOR loop below.\n",
    "# The dictionary will look like this with the bigram and the score from above.\n",
    "# word_dict = {'bigram A': 0.000697411,\n",
    "#             'bigram B': 0.000524882}\n",
    " \n",
    "word_dict = {}\n",
    " \n",
    "listLen = len(scoredList)\n",
    " \n",
    "# Get the bigram and make a contiguous string for the dictionary key. \n",
    "# Set the key to the scored value. \n",
    "for i in range(listLen):\n",
    "    word_dict[' '.join(scoredList[i][0])] = scoredList[i][1]\n",
    " \n",
    "word_dict.update({'Analyst Data':0,'Data Analytics':0})\n",
    "\n",
    "def red_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 99%%, %d%%)\" % random.randint(40, 70)\n",
    "\n",
    "#ASSETS_PATH = BASE_PATH.joinpath(\"assets\").resolve()\n",
    "\n",
    "#wia_mask = np.array(Image.open(ASSETS_PATH.joinpath(\"WIA_logo_heavy.jpg\")))\n",
    "\n",
    "\n",
    "# -----\n",
    " \n",
    "# Set word cloud params and instantiate the word cloud.\n",
    "# The height and width only affect the output image file.\n",
    "WC_height = 500\n",
    "WC_width = 1000\n",
    "WC_max_words = 100\n",
    " \n",
    "wc = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width,stopwords=stopwords, background_color=\"white\")\n",
    " \n",
    "wc.generate_from_frequencies(word_dict)\n",
    " \n",
    "plt.imshow(wc.recolor(color_func=red_color_func, random_state=5), interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wc.to_file(\"wia_title_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import QuadgramAssocMeasures,QuadgramCollocationFinder\n",
    "from operator import itemgetter\n",
    "WNL = nltk.WordNetLemmatizer()\n",
    "\n",
    "df2 = q3_analysis_work['company'].dropna(how='all').astype(str)\n",
    "\n",
    "df2.replace('CMM','CoverMyMeds',inplace=True)\n",
    "df2.replace('The James Cancer Hospital/OSUWMC','The James Cancer Hospital',inplace=True)\n",
    "df2.replace('James Cancer Hospital','The James Cancer Hospital',inplace=True)\n",
    "df2.replace('OSU - The James','The James Cancer Hospital',inplace=True)\n",
    "df2.replace('OSUCCC - The James','The James Cancer Hospital',inplace=True)\n",
    "df2.replace('Ohio State University Wexner Medical Center The James','The Ohio State University Wexner Medical Center',inplace=True)\n",
    "\n",
    "#df2 = \"_\".join(company_title for company_title in df2)\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Generate a word cloud image\n",
    "text_content = [WNL.lemmatize(str(df2)) for t in df2]\n",
    "\n",
    "tokens = nltk.word_tokenize(str(df2))\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# Remove extra chars and remove stop words.\n",
    "text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text]\n",
    "text_content = [word for word in text_content if word not in stopwords]\n",
    " \n",
    "# After the punctuation above is removed it still leaves empty entries in the list.\n",
    "# Remove any entries where the len is zero.\n",
    "text_content = [s for s in text_content if len(s) != 0]\n",
    " \n",
    "# Best to get the lemmas of each word to reduce the number of similar words\n",
    "# on the word cloud. The default lemmatize method is noun, but this could be\n",
    "# expanded.\n",
    "# ex: The lemma of 'characters' is 'character'.\n",
    "#text_content = [WNL.lemmatize(t) for t in text_content]\n",
    " \n",
    "# setup and score the bigrams using the raw frequency.\n",
    "finder = QuadgramCollocationFinder.from_words(text_content)\n",
    "quadgram_measures = QuadgramAssocMeasures()\n",
    "scored = finder.score_ngrams(quadgram_measures.raw_freq)\n",
    "\n",
    "\n",
    "\n",
    "# By default finder.score_ngrams is sorted, however don't rely on this default behavior.\n",
    "# Sort highest to lowest based on the score.\n",
    "scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    " \n",
    "# word_dict is the dictionary we'll use for the word cloud.\n",
    "# Load dictionary with the FOR loop below.\n",
    "# The dictionary will look like this with the bigram and the score from above.\n",
    "# word_dict = {'bigram A': 0.000697411,\n",
    "#             'bigram B': 0.000524882}\n",
    " \n",
    "word_dict = {}\n",
    " \n",
    "listLen = len(scoredList)\n",
    " \n",
    "# Get the bigram and make a contiguous string for the dictionary key. \n",
    "# Set the key to the scored value. \n",
    "for i in range(listLen):\n",
    "    word_dict[' '.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "def blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(240, 99%%, %d%%)\" % random.randint(20, 80)\n",
    "\n",
    "#ASSETS_PATH = BASE_PATH.joinpath(\"assets\").resolve()\n",
    "\n",
    "#wia_mask = np.array(Image.open(ASSETS_PATH.joinpath(\"WIA_logo_heavy.jpg\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    " \n",
    "# Set word cloud params and instantiate the word cloud.\n",
    "# The height and width only affect the output image file.\n",
    "WC_height = 500\n",
    "WC_width = 1000\n",
    "WC_max_words = 100\n",
    " \n",
    "wc = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, background_color=\"white\",prefer_horizontal=0.7)\n",
    " \n",
    "wc.generate_from_frequencies(dict(df2.value_counts()))\n",
    "\n",
    "plt.imshow(wc.recolor(color_func=blue_color_func, random_state=5),interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wc.to_file(\"wia_company_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_analysis_work.technical_expertise.unique()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotly import express as px\n",
    "import plotly\n",
    "\n",
    "df = q3_analysis_work.technical_expertise.value_counts().to_frame().reset_index().rename(columns={\"index\":\"Technical Expertise\",\"technical_expertise\":\"Frequency\"})\n",
    "\n",
    "p = px.histogram(\n",
    "    df,\n",
    "    x='Technical Expertise',\n",
    "    y='Frequency',\n",
    "    histnorm=\"percent\",\n",
    "    histfunc=\"sum\",\n",
    "    color_discrete_map={'':['gray','gray','indianred','gray','gray']},\n",
    "    labels={'Frequency':r'of Attendees (%)'},\n",
    "    category_orders={'Technical Expertise':['Basic Knowledge','Novice','Intermediate','Advanced','Expert']},\n",
    "    title=\"Reported Technical Expertise\")\n",
    "\n",
    "import json\n",
    "json.dumps(p, cls=plotly.utils.PlotlyJSONEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = q3_analysis_work.analytics_experience.value_counts().to_frame().reset_index().rename(columns={\"index\":\"Analytics Experience\",\"analytics_experience\":\"Frequency\"})\n",
    "px.histogram(\n",
    "    df,\n",
    "    x='Analytics Experience',\n",
    "    y='Frequency',\n",
    "    histnorm=\"percent\",\n",
    "    histfunc=\"sum\",\n",
    "    color_discrete_map={'':['gray','indianred','gray','gray']},\n",
    "    labels={'Frequency':r'of Attendees (%)'},\n",
    "    category_orders={'Analytics Experience':['< 1 Year','2-4 Years','5-9 Years','10+ Years']},\n",
    "    title=\"Reported Experience in Analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "px.bar_polar(\n",
    "    q3_analysis_work.industry.value_counts().to_frame() .reset_index().rename(columns={\"index\":\"Industry\",\"industry\":\"Frequency\"}),\n",
    "    r='Frequency',\n",
    "    theta='Industry',\n",
    "    barnorm='percent',\n",
    "    color_discrete_map={'':px.colors.sequential.Sunsetdark},\n",
    "    width=800,\n",
    "    height=550,\n",
    "    start_angle=95,\n",
    "    title='Industry of Attendees (%)'\n",
    "    ).update(layout=dict(images=[dict(\n",
    "        source=Image.open(pathlib.Path(\"testing.ipynb\").parent.resolve().joinpath(\"assets\").joinpath(\"WIA_logo.jpg\")),\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.461, y=0.452,\n",
    "        sizex=0.1, sizey=0.1,\n",
    "        xanchor=\"left\", yanchor=\"bottom\"\n",
    "    )],\n",
    "    title=dict(\n",
    "        x=0.5\n",
    "        ),\n",
    "        font_size=13,\n",
    "        polar=dict(\n",
    "                sector=[89,435],\n",
    "                hole=0.15,\n",
    "                bargap=0.3,\n",
    "                radialaxis=dict(\n",
    "                    gridcolor=\"lightgray\",\n",
    "                    showline=False,\n",
    "                    tickangle=90,\n",
    "                    ticksuffix=\"%\"\n",
    "                ),\n",
    "                angularaxis=dict(\n",
    "                        gridcolor=\"gray\"\n",
    "                    ),\n",
    "                    gridshape=\"circular\",\n",
    "                    bgcolor=\"white\"\n",
    "                    )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "locations = pd.read_csv(DATA_PATH.joinpath(\"q3_location.csv\"))\n",
    "locations.State.value_counts().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "df = locations.State.value_counts().to_frame().reset_index()\n",
    "df['State'] = df['State'].astype(float) / df['State'].astype(float).sum() * 100\n",
    "\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=df['index'], # Spatial coordinates\n",
    "    z = df['State'].astype(int), # Data to be color-coded\n",
    "    locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "    colorscale = 'Reds',\n",
    "    colorbar_title = r\"% of Attendees\",\n",
    "    colorbar_ticksuffix=\"%\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Attendees by State',\n",
    "    geo_scope='north america', # limit map scope to NA\n",
    "    geo = dict(\n",
    "        showframe=False,\n",
    "        projection=go.layout.geo.Projection(type = 'mercator'),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# px.set_mapbox_access_token(\"68f2625c-b463-45ae-8ec3-7f06f83b2073\")\n",
    "# px.choropleth(\n",
    "#     data_frame=locations.State.value_counts().to_frame().reset_index(),\n",
    "#     locations=\"index\",\n",
    "#     locationmode=\"USA-states\",\n",
    "#     scope=\"north america\",\n",
    "#     projection=\"mercator\",\n",
    "#     color=\"State\",\n",
    "#     color_continuous_scale=px.colors.sequential.RdBu\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = locations.State.value_counts().to_frame().reset_index()\n",
    "df['State'] = df['State'].astype(float) / df['State'].astype(float).sum() * 100\n",
    "\n",
    "colors = ['rgb(239,243,255)', 'rgb(189,215,231)', 'rgb(107,174,214)', 'rgb(33,113,181)']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "ff.create_choropleth(\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "#scatter chart of ohio\n",
    "# fig.add_trace(go.Scattergeo(\n",
    "#     lon = df_month['Lon'],\n",
    "#     lat = df_month['Lat'],\n",
    "#     text = df_month['Value'],\n",
    "#     name = months[i],\n",
    "#     marker = dict(\n",
    "#         size=df_month['Value']/50,\n",
    "#         color=colors[i-6],\n",
    "#         line_width=0)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "df_sept = df.query('Month == 9')\n",
    "fig.data[0].update(text = df_sept['Value'].map('{:.0f}'.format).astype(str)+' '+\\\n",
    "                        df_sept['Country'],\n",
    "                     mode = 'markers+text',\n",
    "                     textposition = 'bottom center')\n",
    "\n",
    "\n",
    "fig.add_trace(go.Choropleth(\n",
    "        locationmode='country names',\n",
    "        locations=df_sept['Country'],\n",
    "        z=df_sept['Value'],\n",
    "        text=df_sept['Country'],\n",
    "        colorscale = [[0,'rgb(0, 0, 0)'],[1,'rgb(0, 0, 0)']],\n",
    "        autocolorscale = False,\n",
    "        showscale = False,\n",
    "        geo = 'geo2'\n",
    "    ))\n",
    "fig.add_trace(go.Scattergeo(\n",
    "        lon = [21.0936],\n",
    "        lat = [7.1881],\n",
    "        text = ['Africa'],\n",
    "        mode = 'text',\n",
    "        showlegend = False,\n",
    "        geo = 'geo2'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Ebola cases reported by month in West Africa 2014<br> \\\n",
    "Source: <a href=\"https://data.hdx.rwlabs.org/dataset/rowca-ebola-cases\">\\\n",
    "HDX</a>',\n",
    "    geo = dict(\n",
    "        resolution=50,\n",
    "        scope='africa',\n",
    "        showframe=False,\n",
    "        showcoastlines=True,\n",
    "        showland=True,\n",
    "        landcolor=\"lightgray\",\n",
    "        countrycolor=\"white\" ,\n",
    "        coastlinecolor=\"white\",\n",
    "        projection_type='equirectangular',\n",
    "        lonaxis_range=[ -15.0, -5.0],\n",
    "        lataxis_range=[ 0.0, 12.0],\n",
    "        domain = dict(x=[0, 1], y=[ 0, 1])\n",
    "    ),\n",
    "    geo2 = dict(\n",
    "        scope='africa',\n",
    "        showframe=False,\n",
    "        showland=True,\n",
    "        landcolor=\"lightgray\",\n",
    "        showcountries=False,\n",
    "        domain=dict(x=[ 0, 0.6], y=[ 0, 0.6]),\n",
    "        bgcolor='rgba(255, 255, 255, 0.0)',\n",
    "    ),\n",
    "    legend_traceorder = 'reversed'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = locations[locations.State == \"OH\"]['FIPS'].value_counts().to_frame().reset_index().rename(columns={'FIPS':'Percent','index':'FIPS'})\n",
    "df2['Percent'] = df2['Percent'] / locations.shape[0] * 100\n",
    "\n",
    "colorscale = [\n",
    "    'rgb(193, 193, 193)',\n",
    "    'rgb(239,239,239)',\n",
    "    'rgb(195, 196, 222)',\n",
    "    'rgb(144,148,194)',\n",
    "    'rgb(101,104,168)',\n",
    "    'rgb(65, 53, 132)'\n",
    "]\n",
    "\n",
    "#endpts = list(np.linspace(1,60, len(colorscale) - 1))\n",
    "endpts = list(np.mgrid[min(values):max(values):4j])\n",
    "fips = df2['FIPS'].tolist()\n",
    "values = df2['Percent'].tolist()\n",
    "\n",
    "\n",
    "fig = ff.create_choropleth(\n",
    "    fips=fips, values=values, scope=['Ohio'],\n",
    "    binning_endpoints=endpts, colorscale=colorscale,\n",
    "    show_state_data=True,\n",
    "    show_hover=True,\n",
    "    simplify_county=False,\n",
    "    county_outline={'color': 'rgb(255,255,255)', 'width': 0.5},\n",
    "    round_legend_values=True,\n",
    "    asp = 2.9,\n",
    ")\n",
    "fig.layout.template = None\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = locations[locations.State == \"OH\"]['FIPS'].value_counts().to_frame().reset_index().rename(columns={'FIPS':'Percent','index':'FIPS'})\n",
    "df2['Percent'] = df2['Percent'] / locations.shape[0] * 100\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = locations[locations.State == \"OH\"]['City'].value_counts().to_frame().reset_index().rename(columns={'FIPS':'Percent','index':'FIPS'})\n",
    "df2['Percent'] = df2['Percent'] / locations.shape[0] * 100\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import MapBox\n",
    "loc = MapBox(\"pk.eyJ1IjoibXdhbGtzIiwiYSI6ImNrMDZzdHowdTAxcTAzZG9nMWM5ZW9teXUifQ.rSvhyBymD4tJEPHwJxu0Fw\")\n",
    "latitudes = list()\n",
    "longitudes = list()\n",
    "for row in locations.iterrows():\n",
    "    locdone = loc.geocode(query= dict({'city':str(row[1][0]),'state':str(row[1][1]),'country':str(row[1][2])}))\n",
    "    latitudes.append(locdone.latitude)\n",
    "    longitudes.append(locdone.longitude)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['lat'] = [float(i) for i in latitudes]\n",
    "locations['lon'] = [float(i) for i in longitudes]\n",
    "#latitudes = [float(i) for i in latitudes]\n",
    "#longitudes = [float(i) for i in longitudes]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat = pd.concat([locations.lon.value_counts().to_frame().reset_index().rename(columns={'index':'lon','lon':'count'}).drop('count',axis=1),locations.lat.value_counts().to_frame().reset_index().rename(columns={'index':'lat','lat':'percent'})],axis=1)\n",
    "lonlat['percent'] = lonlat['percent'] / lonlat['percent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_geo(\n",
    "    loc_final,\n",
    "    lat='lat',\n",
    "    lon='lon',\n",
    "    size='percent',\n",
    "    size_max=40,\n",
    "    opacity=0.7,\n",
    "    hover_name='city',\n",
    "    scope='usa',\n",
    "    title='Attendee Proportions by City',\n",
    "    color_discrete_map={'':'orange'},\n",
    "    width=700,\n",
    "    height=500,\n",
    ").update(layout=dict(title=dict(x=0.5),geo=dict(projection_type='albers usa',subunitcolor='gray',subunitwidth=0.5,countrycolor='gray')))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in locations[['lon','lat']].iterrows():\n",
    "     l.append(i[1][0] in longitudes and i[1][1] in latitudes)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_final"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.merge(pd.merge(left=lonlat,right=locations[['City','lon','lat']],on='lat',how='left').drop_duplicates(),locations[['City','lon','lat']],left_on='lon_x',right_on='lon',how='outer')\n",
    "newdf = newdf[['City_y','lon_x','lat_y','percent']]#.rename{columns={'city','lon','lat'}}\n",
    "newdf.columns = ['city','lon','lat','percent']\n",
    "loc_final = newdf.drop_duplicates().reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations[locations.City == 'Cleveland']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_final.to_csv(\"shortcut_loc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}